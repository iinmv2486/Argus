from CNSenv import CommunicatorCNS
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation
from collections import deque
import torch
from torch.nn.functional import softmax
import numpy as np
from Many_to_one_LSTM_network import LSTMClassifier
import pandas as pd
import json



# ğŸ”¹ ì‹œí€€ìŠ¤ë¥¼ ìŒ“ëŠ” Buffer í´ë˜ìŠ¤ ì¶”ê°€
class SequenceBuffer:
    def __init__(self, sequence_length):
        """
        ì‹œí€€ìŠ¤ë¥¼ ì›í•˜ëŠ” ê¸¸ì´ë§Œí¼ ëª¨ì•„ì„œ ë°˜í™˜í•˜ëŠ” ë²„í¼ í´ë˜ìŠ¤
        :param sequence_length: AI ëª¨ë¸ì— ì…ë ¥ìœ¼ë¡œ ë³´ë‚¼ ì‹œí€€ìŠ¤ ê¸¸ì´
        """
        self.sequence_length = sequence_length
        self.buffer = deque(maxlen=sequence_length)  # ìµœëŒ€ ê¸¸ì´ë¥¼ ì œí•œí•˜ì—¬ ì˜¤ë˜ëœ ë°ì´í„° ìë™ ì‚­ì œ

    def add_data(self, new_data):
        """
        ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ì¶”ê°€í•˜ê³ , ì‹œí€€ìŠ¤ê°€ ì¶©ë¶„í•˜ë©´ ë°˜í™˜
        :param new_data: ìƒˆë¡œìš´ ì…ë ¥ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        :return: ì‹œí€€ìŠ¤ ê¸¸ì´ê°€ ì¶©ë¶„í•˜ë©´ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜, ë¶€ì¡±í•˜ë©´ None ë°˜í™˜
        """
        self.buffer.append(new_data)  # ë°ì´í„° ì¶”ê°€
        
        if len(self.buffer) == self.sequence_length:  # ì‹œí€€ìŠ¤ ê¸¸ì´ê°€ ì¶©ì¡±ë˜ë©´ ë°˜í™˜
            return list(self.buffer)  # deque â†’ list ë³€í™˜ í›„ ë°˜í™˜
        
        return None  # ì•„ì§ ë°ì´í„°ê°€ ë¶€ì¡±í•˜ë©´ None ë°˜í™˜

if __name__ == "__main__":
    
    def standardize(y, mean, std):
        """Standardization Function"""
        try:
            if std == 0:
                raise ValueError("Standard deviation cannot be zero for standardization.")
            standardized_value = (y - mean) / std
            return standardized_value
        except Exception as e:
            print(f"Error during standardization: {e}")
            return None


    # CNS í™˜ê²½ ì„¤ì •
    cnsenv = CommunicatorCNS(com_ip='192.168.0.10', com_port=7132)

    feature_importances_path = './src/feature_importances.txt'
    feature_importances = pd.read_csv(feature_importances_path, sep="\t")

    para_list = ['H2CONC', 'VSUMP', 'DSECON', 'ZINST63', 'UFUEL18', 'CBSUMP', 'BLV2', 'XPMOUT', 'CBINTR', 'ZINST28', 'UNORDT', 'XPMBAL', 'BHV51', 'WLETDNO', 'QCSLOC', 'DRADRE', 'ZINST64', 'TCIRCD', 'BPV145I', 'WRHSTM', 'TCVLOC', 'KFIRST', 'ZCVLP', 'UAVLEG1', 'ZINST54', 'CAUTRDC', 'ZINST61', 'UCOOL', 'WCPLN1', 'WRHDRNA', 'WHPRH', 'ZVCT', 'FEICDP', 'ZINST51', 'UCWOUT', 'UCTMT', 'HHPRH', 'PHPTIN', 'ZINST102', 'UDHOCO2', 'WHPHBYA', 'BFWMAI', 'DECH', 'WAFWTK', 'WCHARGT', 'PCNDS', 'HRHTR', 'FHTRDP', 'KZROD25', 'UCNDTK', 'ZINST36', 'ZINST105', 'ZSGNOR2', 'ULPHCD', 'XSEBAL', 'ZINST78', 'UCOLEG1', 'UFWLN1', 'ZSGN3', 'DCTMT', 'URHTUB', 'UCOLEGM', 'BPV145', 'WFDW', 'PCDP', 'CBRWST', 'BFV489', 'WNETCH', 'CBVCT', 'UFUEL17', 'UCCWSW', 'WCSLOC']
    
    scaler_parameters_path = './src/scaler_parameters.json'

    # í‘œì¤€í™” íŒŒë¼ë¯¸í„° ë¡œë“œ
    try:
        with open(scaler_parameters_path, 'r') as f:
            scaler_parameters = json.load(f)
    except FileNotFoundError:
        raise FileNotFoundError("í‘œì¤€í™” íŒŒë¼ë¯¸í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.")

    mean = scaler_parameters['mean']
    std = scaler_parameters['std']
    
    # ë”•ì…”ë„ˆë¦¬ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    para_mean = np.array(list(mean.values()))
    para_std = np.array(list(std.values()))

    print("Hello", para_mean.shape, para_std.shape)

    # AI ë„¤íŠ¸ì›Œí¬ ë¡œë“œ
    # ëª¨ë¸ ì´ˆê¸°í™”ì— í•„ìš”í•œ íŒŒë¼ë¯¸í„° ì„¤ì •
    input_dim = 72        # ê° ì‹œì ì—ì„œì˜ ì…ë ¥ í”¼ì²˜ ìˆ˜
    hidden_dim = 50       # LSTMì˜ ì€ë‹‰ ìƒíƒœ ì°¨ì›
    num_classes = 5       # ë¶„ë¥˜ í´ë˜ìŠ¤ ìˆ˜
    num_layers = 2        # LSTM ë ˆì´ì–´ ìˆ˜

    # CUDA ì¥ì¹˜ ì„¤ì •
    Net_origin = LSTMClassifier(input_dim=input_dim, hidden_dim=hidden_dim, num_classes=num_classes, num_layers=num_layers)
    Net_origin.load_network('./models/Many_to_one_LSTM.pt')

    # ë°ì´í„° ì €ì¥ ë³€ìˆ˜
    sim_time = []
    net_out = { "NORMAL": [], "LOCA": [], "SGTR": [], "MSLB_in": [], "MSLB_out": [] }
    NetDiag = ''

    # ì‹œí€€ìŠ¤ ê¸¸ì´ ì„¤ì • ë° ë²„í¼ ìƒì„±
    sequence_length = 5
    buffer = SequenceBuffer(sequence_length)

    # ê·¸ë˜í”„ ì´ˆê¸°í™”
    fig, ax = plt.subplots(figsize=(8,6))
    ax.set_position([0.10, 0.30, 0.85, 0.65])

    # ê·¸ë˜í”„ ì„  ìƒì„±
    colors = {'NORMAL': 'black', 'LOCA': 'blue', 'SGTR': 'red', 'MSLB_in': 'green', 'MSLB_out': 'magenta'}
    lines = {key: ax.plot([], [], label=key, color=colors[key], linewidth=2.5)[0] for key in net_out}

    # ë ˆì´ë¸” ìƒì„±
    Label_Dig = fig.text(0.1, 0.15, f'Diagnosis : {NetDiag}', wrap=True)
    Labels = {
        "NORMAL": fig.text(0.1, 0.10, '', wrap=True, backgroundcolor='#eefade'),
        "LOCA": fig.text(0.1, 0.05, '', wrap=True, backgroundcolor='#eefade'),
        "SGTR": fig.text(0.1, 0.01, '', wrap=True, backgroundcolor='#eefade'),
        "MSLB_in": fig.text(0.5, 0.10, '', wrap=True, backgroundcolor='#eefade'),
        "MSLB_out": fig.text(0.5, 0.05, '', wrap=True, backgroundcolor='#eefade')
    }

    ax.set_title("AI Result")
    ax.set_xlabel("Time (sim. tick)")
    ax.set_ylabel("SoftMax Out")
    ax.axhline(y=0.9, linestyle=':', color='black')
    ax.legend(loc='upper center', ncol=5, bbox_to_anchor=(0.5, 1))

    with open('GP_Result_team.csv', 'w') as f:
        f.write(f'Tick,NORMAL,LOCA,SGTR,MSLB_in,MSLB_out,Diagnosis,Diagnosis_label\n')

    def update(frame):
        """ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸"""
        is_updated = cnsenv.read_data()

        if is_updated:
            db_line = np.array([cnsenv.mem[para]['Val'] for j, para in enumerate(para_list)])
            print("line", db_line.shape)
            # db_line_normalized = standardize(db_line,para_mean,para_std)
            db_line_normalized  = (db_line - para_mean) / para_std
            print("norm", db_line_normalized)
            # ì‹œí€€ìŠ¤ ë²„í¼ì— ë°ì´í„° ì¶”ê°€
            sequence = buffer.add_data(db_line_normalized)
            print(np.array(sequence).shape)

            if sequence:
                print(f"ë°ì´í„° {sequence_length}ê°œê°€ ìŒ“ì˜€ìŒ! AI ëª¨ë¸ì— ì „ë‹¬í•  ì‹œí€€ìŠ¤:")

                # Torch Tensor ë³€í™˜ (batch ì°¨ì› ì¶”ê°€)
                inputs = torch.tensor(sequence, dtype=torch.float32).unsqueeze(0)  # (1, seq_length, input_size)
                print(inputs.size())

                # ëª¨ë¸ ì˜ˆì¸¡
                with torch.no_grad():  # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ë°©ì§€
                    out = Net_origin.forward(inputs)

                print(out)
                # Softmax ì ìš© í›„ argmaxë¡œ ê°€ì¥ í™•ë¥  ë†’ì€ í´ë˜ìŠ¤ ì„ íƒ
                soft_out = softmax(out, dim=1).tolist()[0]  # í™•ë¥ ê°’ ë¦¬ìŠ¤íŠ¸ [5ê°œ]
                pred_idx = torch.argmax(torch.tensor(soft_out)).item()
                NetDiag = ["NORMAL", "LOCA", "SGTR", "MSLB inside", "MSLB outside"][pred_idx]

                soft_out_w = list(soft_out)
                soft_out_w.insert(0,str(cnsenv.mem["KCNTOMS"]["Val"]))
                soft_out_w = ','.join(map(str,soft_out_w))
                with open('GP_Result_team.csv','a') as f:
                    f.write(f'{soft_out_w},{str(pred_idx)},{str(NetDiag)}\n')

                # CNS ì‹œë®¬ë ˆì´ì…˜ ì‹œê°„
                current_time = cnsenv.mem['KCNTOMS']['Val']
                sim_time.append(current_time)

                # í™•ë¥  ì €ì¥
                for i, key in enumerate(net_out.keys()):
                    net_out[key].append(soft_out[i])

                # ê·¸ë˜í”„ ë°ì´í„° ì—…ë°ì´íŠ¸
                for key, line in lines.items():
                    line.set_data(sim_time, net_out[key])

                # ë ˆì´ë¸” ì—…ë°ì´íŠ¸
                Label_Dig.set_text(f'Diagnosis : {NetDiag}')
                for i, key in enumerate(Labels.keys()):
                    print(i, key, net_out[key][-1] * 100)
                    Labels[key].set_text(f'{key} : {net_out[key][-1] * 100:.2f}%')

                # ê·¸ë˜í”„ ìŠ¤ì¼€ì¼ ì¡°ì •
                ax.relim()
                ax.autoscale_view() 

            else:
                remaining = sequence_length - len(buffer.buffer)  # ë‚¨ì€ ë°ì´í„° ê°œìˆ˜ í™•ì¸
                print(f"ë°ì´í„°ê°€ ë¶€ì¡±í•¨. {remaining}ê°œ ë” í•„ìš”í•¨...")

    # ì• ë‹ˆë©”ì´ì…˜ ì‹¤í–‰
    ani = FuncAnimation(fig, update, interval=100, cache_frame_data=False)
    plt.show()
